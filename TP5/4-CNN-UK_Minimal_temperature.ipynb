{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNStK09J1Iy/g83iTJjF0b+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/CHPS0704/blob/main/TP5/4-CNN-UK_Minimal_temperature.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UK minimal temperature prediction\n",
        "\n",
        "Uploader le fichier \"MET_Office_Weather_Data.xlsx\""
      ],
      "metadata": {
        "id": "JAnNR6EGcFYJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5hPxMY-cDVO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import copy\n",
        "import random\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "features = 120\n",
        "ts_len = 60\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def raw_time_series():\n",
        "    ts_df = pd.read_csv('MET_Office_Weather_Data.csv',delimiter=';')\n",
        "    ts = ts_df.loc[ts_df['station'] == 'oxford']['tmin'].tolist()\n",
        "    return ts"
      ],
      "metadata": {
        "id": "lfEbip-tcl5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolated_time_series():\n",
        "    ts_df = pd.read_csv('MET_Office_Weather_Data.csv',delimiter=';')\n",
        "    ts = ts_df.loc[ts_df['station'] == 'oxford']['tmin']\\\n",
        "        .interpolate().dropna().tolist()\n",
        "    return ts"
      ],
      "metadata": {
        "id": "zXEy_jJWdjZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_window(ts, features):\n",
        "    X = []\n",
        "    Y = []\n",
        "\n",
        "    for i in range(features + 1, len(ts) + 1):\n",
        "        X.append(ts[i - (features + 1):i - 1])\n",
        "        Y.append([ts[i - 1]])\n",
        "\n",
        "    return X, Y\n"
      ],
      "metadata": {
        "id": "WZwz8cHTcu-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_datasets(features, test_len):\n",
        "    ts = interpolated_time_series()\n",
        "    X, Y = sliding_window(ts, features)\n",
        "\n",
        "    X_train, Y_train, X_test, Y_test = X[0:-test_len], \\\n",
        "                                       Y[0:-test_len], \\\n",
        "                                       X[-test_len:], \\\n",
        "                                       Y[-test_len:]\n",
        "\n",
        "    train_len = round(len(ts) * 0.7)\n",
        "\n",
        "    X_train, X_val, Y_train, Y_val = X_train[0:train_len],\\\n",
        "                                     X_train[train_len:],\\\n",
        "                                     Y_train[0:train_len],\\\n",
        "                                     Y_train[train_len:]\n",
        "\n",
        "    x_train = torch.tensor(data = X_train)\n",
        "    y_train = torch.tensor(data = Y_train)\n",
        "\n",
        "    x_val = torch.tensor(data = X_val)\n",
        "    y_val = torch.tensor(data = Y_val)\n",
        "\n",
        "    x_test = torch.tensor(data = X_test)\n",
        "    y_test = torch.tensor(data = Y_test)\n",
        "\n",
        "\n",
        "\n",
        "    return x_train, x_val, x_test, y_train, y_val, y_test"
      ],
      "metadata": {
        "id": "MeZQD6xccxr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "\n",
        "class HwesPredictor(torch.nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        last_values = []\n",
        "        for r in x.tolist():\n",
        "            model = ExponentialSmoothing(r,\n",
        "                                         trend = None,\n",
        "                                         seasonal = \"add\",\n",
        "                                         seasonal_periods = 12\n",
        "                                         )\n",
        "            results = model.fit()\n",
        "            forecast = results.forecast()\n",
        "            last_values.append([forecast[0]])\n",
        "        return torch.tensor(data = last_values)"
      ],
      "metadata": {
        "id": "3h3AwyQxe3zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO -> Remplir la classe suivante pour avoir un predicteur SARIMA\n",
        "\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "class SarimaxPredictor(torch.nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "    # TODO - vous pouvez vous inspirer de l'exemple \"HWESPredictor\" précédent\n",
        "    # la doc de SARIMAX se trouve ici : https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html\n",
        "    # suggestion : utiliser order = (1, 1, 1) et seasonal_order = (1, 1, 1, 12)\n",
        "\n",
        "\n",
        "        return torch.tensor(data = last_values)"
      ],
      "metadata": {
        "id": "p8Lu5Ry8eRm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DL(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_inp, l_1, l_2, conv1_out, conv1_kernel, conv2_kernel, drop1 = 0):\n",
        "        super(DL, self).__init__()\n",
        "        conv1_out_ch = conv1_out\n",
        "        conv2_out_ch = conv1_out * 2\n",
        "        conv1_kernel = conv1_kernel\n",
        "        conv2_kernel = conv2_kernel\n",
        "        self.dropout_lin1 = drop1\n",
        "\n",
        "        self.pool = torch.nn.MaxPool1d(kernel_size = 2)\n",
        "\n",
        "        self.conv1 = torch.nn.Conv1d(in_channels = 1, out_channels = conv1_out_ch, kernel_size = conv1_kernel,\n",
        "                                     padding = conv1_kernel - 1)\n",
        "\n",
        "        self.conv2 = torch.nn.Conv1d(in_channels = conv1_out_ch, out_channels = conv2_out_ch,\n",
        "                                     kernel_size = conv2_kernel,\n",
        "                                     padding = conv2_kernel - 1)\n",
        "\n",
        "        feature_tensor = self.feature_stack(torch.Tensor([[0] * n_inp]))\n",
        "        self.lin1 = torch.nn.Linear(feature_tensor.size()[1], l_1)\n",
        "        self.lin2 = torch.nn.Linear(l_1, l_2)\n",
        "        self.lin3 = torch.nn.Linear(l_2, 1)\n",
        "\n",
        "    def feature_stack(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = F.relu(self.pool(self.conv1(x)))\n",
        "        x = F.relu(self.pool(self.conv2(x)))\n",
        "        x = x.flatten(start_dim = 1)\n",
        "        return x\n",
        "\n",
        "    def fclassification_stack(self, x):\n",
        "        x1 = F.dropout(F.relu(self.lin1(x)), p = self.dropout_lin1)\n",
        "        x2 = F.relu(self.lin2(x1))\n",
        "        y = self.lin3(x2)\n",
        "        return y\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_stack(x)\n",
        "        y = self.fclassification_stack(x)\n",
        "        return y"
      ],
      "metadata": {
        "id": "323zhzFWdt_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_val, x_test, y_train, y_val, y_test = get_training_datasets(features, ts_len)\n",
        "\n",
        "sarima_predictor = SarimaxPredictor()\n",
        "hwes_predictor = HwesPredictor()\n",
        "\n",
        "net = DL( n_inp = features,\n",
        "    l_1 = 400,\n",
        "    l_2 = 48,\n",
        "    conv1_out = 6,\n",
        "    conv1_kernel = 36,\n",
        "    conv2_kernel = 12,\n",
        "    drop1 = .1\n",
        ")\n",
        "\n",
        "net.train()\n",
        "\n",
        "optimizer = torch.optim.Adam(params = net.parameters())\n",
        "abs_loss = torch.nn.L1Loss()\n",
        "\n",
        "\n",
        "best_model = None\n",
        "min_val_loss = sys.maxsize\n",
        "\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "for t in range(150):\n",
        "\n",
        "    prediction = net(x_train)\n",
        "    loss = abs_loss(prediction, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    val_prediction = net(x_val)\n",
        "    val_loss = abs_loss(val_prediction, y_val)\n",
        "\n",
        "    training_loss.append(loss.item())\n",
        "    validation_loss.append(val_loss.item())\n",
        "\n",
        "    if val_loss.item() < min_val_loss:\n",
        "        best_model = copy.deepcopy(net)\n",
        "        min_val_loss = val_loss.item()\n",
        "        #print(t,\" - meilleur\")\n",
        "\n",
        "    if t % 10 == 0:\n",
        "        print(f'epoch {t}: train - {round(loss.item(), 4)}, val: - {round(val_loss.item(), 4)}')\n",
        "\n",
        "best_model.eval()"
      ],
      "metadata": {
        "id": "7EHbtJVIgAHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl_prediction = best_model(x_test)\n",
        "sarima_prediction = sarima_predictor(x_test)\n",
        "hwes_prediction = hwes_predictor(x_test)\n",
        "\n",
        "dl_abs_loss = round(abs_loss(dl_prediction, y_test).item(), 4)\n",
        "sarima_abs_loss = round(abs_loss(sarima_prediction, y_test).item(), 4)\n",
        "hwes_abs_loss = round(abs_loss(hwes_prediction, y_test).item(), 4)\n",
        "\n",
        "print('===')\n",
        "print('Results on Test Dataset')\n",
        "print(f'DL Loss: {dl_abs_loss}')\n",
        "print(f'SARIMA Loss: {sarima_abs_loss}')\n",
        "print(f'HWES Loss: {hwes_abs_loss}')"
      ],
      "metadata": {
        "id": "SwHZffiSiktm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Training progress\")\n",
        "plt.plot(training_loss, label = 'training loss')\n",
        "plt.plot(validation_loss, label = 'validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6H-ku-HenmE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ecart entre actuel et prédit"
      ],
      "metadata": {
        "id": "22D-BSM9pLaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_n = len(y_test)\n",
        "dl_abs_dev = (dl_prediction - y_test).abs_()\n",
        "sarima_abs_dev = (sarima_prediction - y_test).abs_()\n",
        "hwes_abs_dev = (hwes_prediction - y_test).abs_()\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "ax1 = fig.add_subplot(311)\n",
        "ax2 = fig.add_subplot(312)\n",
        "ax3 = fig.add_subplot(313)\n",
        "\n",
        "ax1.set_title(f'Deep Learning Model: {dl_abs_loss}')\n",
        "ax1.bar(list(range(test_n)), dl_abs_dev.view(test_n).tolist(), color = 'g')\n",
        "\n",
        "ax2.set_title(f'SARIMA Model: {sarima_abs_loss}')\n",
        "ax2.bar(list(range(test_n)), sarima_abs_dev.view(test_n).tolist(), color = 'r')\n",
        "\n",
        "ax3.set_title(f'HWES Model: {hwes_abs_loss}')\n",
        "ax3.bar(list(range(test_n)), hwes_abs_dev.view(test_n).tolist(), color = 'brown')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w0afpiRNnmzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge\n",
        "\n",
        "Modifier votre réseau de neurones pour obtenir un écart moyen inférieur à 1.25"
      ],
      "metadata": {
        "id": "Dr_HgV3GptFl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "THvFyDeqnu3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}