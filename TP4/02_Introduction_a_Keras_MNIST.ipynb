{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "TP1_Some_basics_about_learning_process.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/CHPS0704/blob/main/TP4/02_Introduction_a_Keras_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construire un modèle de réseaux de neurones avec Keras\n",
        "\n",
        "Keras est une bibliothèque \"haut niveau\" utilisée pour simplifier la description de modèles de réseaux de neurones sur Tensorflow (bibliothèque IA de Google). L'avantage surtout est de pouvoir utiliser des GPU pour accélérer le calcul.\n",
        "\n",
        "Le travail avec Keras suit un cheminement similaires à celui avec Scikit-Learn, mais il y a quelques différences à retenir."
      ],
      "metadata": {
        "id": "EpQRZSswWmWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import numpy as np\n",
        "print (tf.__version__)\n",
        "print(keras.__version__)\n"
      ],
      "metadata": {
        "id": "nO3TA6f7WmWJ",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le paragraphe suivant vous avez certainement eu un message d'erreur indiquant que vous n'avez pas des GPU. Dans ce cas, Keras utilisera la CPU de la machine.\n",
        "\n",
        "## Chargement de données\n",
        "\n",
        "Tout comme Scikit-Learn, Keras a aussi un ensemble de datasets prêt à utilisation pour des exemples. Dans le cas suivant, nous allons charger le dataset MNIST (écriture à la main) et le séparer en deux groupes : Train et Test. Les données de validation (vérification pendant l'entraînement) seront séparés du groupe Train plus tard."
      ],
      "metadata": {
        "id": "5eYo87D2WmWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "q2eLH6ACWmWJ",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape\n",
        "x_test.shape\n",
        "y_test.shape"
      ],
      "metadata": {
        "id": "odgDUCJSWmWJ",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "iOXq4OkDWmWK",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les données de MNIST se présentent sous la forme d'images 28x28 pixels, avec 256 tons de gris. Les labels (`y_train`, par exemple) correspondent aux caractères représentés : les chiffres 0 à 9.\n",
        "\n",
        "Le paragraphe suivant définit une fonction permettant de visualiser ce dataset."
      ],
      "metadata": {
        "id": "8cNaXhPsWmWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(x,y=None, indices='all', columns=12, x_size=1, y_size=1,\n",
        "                colorbar=False, y_pred=None, cm='binary', norm=None, y_padding=0.35, spines_alpha=1,\n",
        "                fontsize=20, interpolation='lanczos', save_as='auto'):\n",
        "    \"\"\"\n",
        "    Show some images in a grid, with legends\n",
        "    args:\n",
        "        x             : images - Shapes must be (-1,lx,ly) (-1,lx,ly,1) or (-1,lx,ly,3),(-1,1,lx,ly) or (-1,3,lx,ly)\n",
        "        y             : real classes or labels or None (None)\n",
        "        indices       : indices of images to show or 'all' for all ('all')\n",
        "        columns       : number of columns (12)\n",
        "        x_size,y_size : figure size (1), (1)\n",
        "        colorbar      : show colorbar (False)\n",
        "        y_pred        : predicted classes (None)\n",
        "        cm            : Matplotlib color map (binary)\n",
        "        norm          : Matplotlib imshow normalization (None)\n",
        "        y_padding     : Padding / rows (0.35)\n",
        "        spines_alpha  : Spines alpha (1.)\n",
        "        font_size     : Font size in px (20)\n",
        "        save_as       : Filename to use if save figs is enable ('auto')\n",
        "    returns:\n",
        "        nothing\n",
        "    \"\"\"\n",
        "    if indices=='all': indices=range(len(x))\n",
        "    if norm and len(norm) == 2: norm = matplotlib.colors.Normalize(vmin=norm[0], vmax=norm[1])\n",
        "    draw_labels = (y is not None)\n",
        "    draw_pred   = (y_pred is not None)\n",
        "    # Torch Tensor ?\n",
        "    if y.__class__.__name__      == 'Tensor': y=y.numpy()\n",
        "    if y_pred.__class__.__name__ == 'Tensor': y_pred=y_pred.detach().numpy()\n",
        "\n",
        "    rows        = math.ceil(len(indices)/columns)\n",
        "    fig=plt.figure(figsize=(columns*x_size, rows*(y_size+y_padding)))\n",
        "    n=1\n",
        "    for i in indices:\n",
        "        axs=fig.add_subplot(rows, columns, n)\n",
        "        n+=1\n",
        "        # ---- Shape is (lx,ly)\n",
        "        if len(x[i].shape)==2:\n",
        "            xx=x[i]\n",
        "        # ---- Shape is (lx,ly,c) or (c,lx,ly)\n",
        "        if len(x[i].shape)==3:\n",
        "            if x[i].__class__.__name__ == 'Tensor':\n",
        "               (c,lx,ly)=x[i].shape\n",
        "               if c==1:\n",
        "                   xx=x[i].permute(1,2,0).numpy().reshape(lx,ly)\n",
        "               else:\n",
        "                   xx=x[i].permute(1,2,0).numpy() #---> (lx,ly,n)\n",
        "            else:\n",
        "                (lx,ly,c)=x[i].shape\n",
        "                if c==1:\n",
        "                    xx=x[i].reshape(lx,ly)\n",
        "                else:\n",
        "                    xx=x[i]\n",
        "\n",
        "        img=axs.imshow(xx,   cmap = cm, norm=norm, interpolation=interpolation)\n",
        "        axs.spines['right'].set_visible(True)\n",
        "        axs.spines['left'].set_visible(True)\n",
        "        axs.spines['top'].set_visible(True)\n",
        "        axs.spines['bottom'].set_visible(True)\n",
        "        axs.spines['right'].set_alpha(spines_alpha)\n",
        "        axs.spines['left'].set_alpha(spines_alpha)\n",
        "        axs.spines['top'].set_alpha(spines_alpha)\n",
        "        axs.spines['bottom'].set_alpha(spines_alpha)\n",
        "        axs.set_yticks([])\n",
        "        axs.set_xticks([])\n",
        "        if draw_labels and not draw_pred:\n",
        "            axs.set_xlabel(y[i],fontsize=fontsize)\n",
        "        if draw_labels and draw_pred:\n",
        "            if y[i]!=y_pred[i]:\n",
        "                axs.set_xlabel(f'{y_pred[i]} ({y[i]})',fontsize=fontsize)\n",
        "                axs.xaxis.label.set_color('red')\n",
        "            else:\n",
        "                axs.set_xlabel(y[i],fontsize=fontsize)\n",
        "        if colorbar:\n",
        "            fig.colorbar(img,orientation=\"vertical\", shrink=0.65)\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "DlffLJa0u_Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons donc afficher l'un des caractères du groupe Train (celui à la position 27) mais aussi tout la plage entre les données 5 et 41. Remarquez le \"label\" correspondant sous chaque image."
      ],
      "metadata": {
        "id": "AW52c1d3O5T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[27])\n",
        "plot_images(x_train, y_train, indices=[27],  x_size=5,y_size=5, colorbar=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "rsjX4ZScO5T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_images(x_train, y_train, range(5,41), columns=12)"
      ],
      "metadata": {
        "id": "6UZASiFq7Nfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 - Normalisation des données**\n",
        "\n",
        "Les valeurs de base son des entiers entre 0 et 255 pour représenter les 256 tons de gris. La majorité des algorithmes utilisent des valeurs réels, de préférence dans la fourchette 0 à 1 ou -1 à 1.\n",
        "\n",
        "Les paragraphes suivantes modifient le type des données (`float32`) puis font une normalisation simple (diviser la valeur par `max`, qui dans ce cas doît être de 255). Bien sûr, d'autres méthodes de normalisation plus élaborées sont possibles, mais ça suffit pour l'instant."
      ],
      "metadata": {
        "id": "pSIUUpzqO5UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')"
      ],
      "metadata": {
        "trusted": true,
        "id": "RQp1Q6ADO5UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before normalization : Min={}, max={}'.format(x_train.min(),x_train.max()))\n",
        "\n",
        "xmax=x_train.max()\n",
        "x_train = x_train / xmax\n",
        "x_test  = x_test  / xmax\n",
        "\n",
        "print('After normalization  : Min={}, max={}'.format(x_train.min(),x_train.max()))"
      ],
      "metadata": {
        "trusted": true,
        "id": "6QWNy_ePO5UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 - Transformer les données catégoriques**\n",
        "Lorsqu'on a des données catégoriques (texte ou numéros), il faut les transformer afin d'éviter des mauvaises compréhensions de la part de l'algorithme (par exemple, supposer que une classe 2 vient toujours après une classe 1). Dans notre cas, nous allons transformer les classes 0 à 9 en représentations numériques (similaire à HotOneEncoder de Sklearn), afin de rendre indépendantes ces classes."
      ],
      "metadata": {
        "id": "qz-IyPVqO5UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_train_hotone = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test_hotone = keras.utils.to_categorical(y_test, num_classes=10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "AgI0qRWrO5UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_hotone"
      ],
      "metadata": {
        "id": "5RGjh84XWmWK",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## La Création d'un modèle\n",
        "\n",
        "Keras a plusieurs modes permettant la création de modèles de réseaux de neurones. Dans ce cas, nous allons utiliser l'API `Sequential` qui permet de décrire couche par couche du réseau et les empiler (grâce à `add()`).\n",
        "\n",
        "Nous allons faire un modèle simple avec des réseaux denses (totalement connectés). La première couche définit la taille de l'entrée (les 784 valeurs reçus du dataset), les autres utilisent par défaut la taille de la sortie de la couche précédente. Egalement, nous indiquons que chaque couche comptera avec 10 neurones.\n",
        "\n",
        "Finalemen, remarquez qu'on utilise deux types de fonction d'activation, sigmoid et softmax.\n",
        "Pour simplifier la description, sigmoid donne une probabilité entre 0 et 1, alors que Softmax affiche \"1\" sur la sortie avec la plus grande probabilité et \"0\" sur les autres. C'est pour cela qu'on utilise Softmax à la sortie, ça permet d'avoir un résultat plutôt qu'une liste de probabilités."
      ],
      "metadata": {
        "id": "hXtTapcTO5UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Sequential\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "\n",
        "# Declaration du modèle en Keras\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Input((28,28)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(30, activation='sigmoid'))\n",
        "model.add(layers.Dense(20, activation='sigmoid'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "# résumé du modèle\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "U9XLxq-BWmWK",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entraînement du modèle\n",
        "\n",
        "Une fois défini le modèle, il faut l'entraîner avec les données.\n",
        "Le paragraphe suivant définit les hyperparamètres du modèle, dont le `batch_size`(taille des sous-ensembles utilisés dans la descente de gradient), le nombre d'epochs (parcours de l'ensemble de données d'entraînement).\n",
        "\n",
        "L'appel à compile génère le graphe d'exécution (un réseau de neurones = un graphe) et indique aussi qu'on utilise le modèle de descente de gradient SGD, que la métrique utilisée est l'accuracy (métrique qui correspond à (TP+TN)/(TP+TN+FP+FN)), et que la fonction de perte est la `categorical_crossentropy`, une fonction qui compare les probabilités pour des labels catégoriques."
      ],
      "metadata": {
        "id": "pg4Axb4GO5UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "#num_classes = 10\n",
        "epochs= 20\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',  optimizer='SGD',  metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "32wR8ClWWmWL",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalement, on lance l'entraînement. Remarquez aussi qu'on n'a pas crée des données Validation avant, on le fera **sur place** en réservant 10% des données de Train (appel à `validation_split=0.1`).\n",
        "\n",
        "Comme le dataset est simple, on peut faire l'entraînement même sans un GPU."
      ],
      "metadata": {
        "id": "1A-EfnLBO5UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train_hotone, batch_size=batch_size, epochs=epochs, validation_split=0.1,verbose=1 )\n",
        "\n",
        "#verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
        "# Je vous invite à lire la documentation : https://keras.io/models/sequential/"
      ],
      "metadata": {
        "id": "Jng3P9z1WmWL",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les paragraphes suivants nous permettent de voir comment le modèle améliore sa performance au fil des epochs"
      ],
      "metadata": {
        "id": "Y-bcqc4zO5UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history, figsize=(8,6),\n",
        "                 plot={\"Accuracy\":['accuracy','val_accuracy'], 'Loss':['loss', 'val_loss']}):\n",
        "    \"\"\"\n",
        "    Show history\n",
        "    args:\n",
        "        history: history\n",
        "        figsize: fig size\n",
        "        plot: list of data to plot : {<title>:[<metrics>,...], ...}\n",
        "    \"\"\"\n",
        "    fig_id=0\n",
        "    for title,curves in plot.items():\n",
        "        plt.figure(figsize=figsize)\n",
        "        plt.title(title)\n",
        "        plt.ylabel(title)\n",
        "        plt.xlabel('Epoch')\n",
        "        for c in curves:\n",
        "            plt.plot(history.history[c])\n",
        "        plt.legend(curves, loc='upper left')\n",
        "\n",
        "        plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "mrjA4FOuO5UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history, figsize=(6,4))"
      ],
      "metadata": {
        "trusted": true,
        "id": "QKhT0-sOO5UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enfin, on peut estimer la performance du modèle avec les données Test.\n",
        "\n",
        "Comparez ces valeur avec ceux de l'entraînement (`val_loss` et `val_accuracy`\n",
        " ci-dessus)."
      ],
      "metadata": {
        "id": "TGLQHMDPO5UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test_hotone,verbose=0)\n",
        "\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "qTbNPwg6WmWL",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ces résultats montrent que, étonemment, le modèle se porte mieux avec le groupe de validation (val_accuracy), c'est intéressant 👍."
      ],
      "metadata": {
        "id": "zMgRWzJsO5UC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RmZYQe-HzKYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quelques erreurs\n",
        "\n",
        "Si vous avez fait attention, on n'a toujours pas touché le dataset `x_test/y_test`, vu qu'on a fait l'entraînement avec 90% et validé avec 10% du x_train.\n",
        "\n",
        "On peut donc utiliser x_test comme \"nouvelle donnée\" et vérifier si notre modèle rend bien les réponses.\n",
        "\n",
        "Dans le prochain paragraph, nous allons donc produire une estimation `y_pred` à partir de `x_test`. Comme la sortie du modèle est un array de 10 positions (notre `y_train_hotone`), on passe ce résultat à `np.argmax()` pour obtenir juste le numéro de la classe."
      ],
      "metadata": {
        "id": "RoHS31T-ySZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d'abord, on utilise le modèle pour faire une prévision sur l'ensemble de test\n",
        "# Ça retourne une liste avec 10 colonnes (une par sortie possible).\n",
        "y_pred_hotone = model.predict(x_test)\n",
        "\n",
        "# avec la fonction argmax, on ne garde que l'index de la colonne avec la plus grande valeur\n",
        "y_pred    = np.argmax(y_pred_hotone, axis=-1)"
      ],
      "metadata": {
        "id": "94ywQWlvzOoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant, on affiche quelques éléments, avec la valeur prédite et la valeur attendue (entre parenthèses). Vous pouvez vérifier que quelques prédictions sont incorrectes."
      ],
      "metadata": {
        "id": "F661igG7zV6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plot_images(x_test, y_test, range(0,200), columns=12, x_size=1, y_size=1, y_pred=y_pred)"
      ],
      "metadata": {
        "id": "u_TTk0YGWmWL",
        "jupyter": {
          "source_hidden": true
        },
        "tags": []
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice :\n",
        "Avec ce modèle basique, on a obtenu un taux d'accuracy d'environ 80%.\n",
        "- Essayer d'améliorer la performence du modèle, en modifiant les fonctions d'activation, ou/et en n ajoutant le nombre de neurones et des couches intermédiaires.\n",
        "\n"
      ],
      "metadata": {
        "id": "0G4jaLW6WmWL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EbsfpaiazW2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus : Quelques exemples de fonctions d'activation de base\n",
        "\n"
      ],
      "metadata": {
        "id": "8_w-7ZF9WmWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation functions"
      ],
      "metadata": {
        "id": "Kd01IKzdWmWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "x = np.arange(-6, 6, 0.1)"
      ],
      "metadata": {
        "id": "ShyJUe2VWmWE",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear  :  Fonction d'activation linéaire\n",
        "\n",
        "C'est une fonction simple de la forme: f(x) = ax ou f(x) = x. En gros, l'entrée passe à la sortie sans une très grande modification ou alors sans aucune modification."
      ],
      "metadata": {
        "id": "K_RTo7D7WmWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear(x):\n",
        "    a = []\n",
        "    for item in x:\n",
        "        a.append(item)\n",
        "    return a\n",
        "\n",
        "y = linear(x)\n",
        "\n",
        "plt.plot(x,y)\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yoxBCvBSWmWF",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sigmoid\n",
        "\n",
        "En mathématiques, la fonction sigmoïde (dite aussi courbe en S) est définie par :\n",
        "\n",
        "$$ f(x)=\\frac{1}{1 + e^{- x}}$$ pour tout réel x ;\n",
        "\n",
        "\n",
        "Le but premier de la fonction est de réduire la valeur d'entrée pour la réduire entre 0 et 1. En plus d'exprimer la valeur sous forme de probabilité, si la valeur en entrée est un très grand nombre positif, la fonction convertira cette valeur en une probabilité de 1. A l'inverse, si la valeur en entrée est un très grand nombre négatif, la fonction convertira cette valeur en une probabilité de 0. D'autre part, l'équation de la courbe est telle que, seules les petites valeurs influent réellement sur la variation des valeurs en sortie.\n",
        "\n",
        "La fonction Sigmoïde a plusieurs défaults:\n",
        "\n",
        "- Elle n'est pas centrée sur zéro, c'est à dire que des entrées négatives peuvent engendrer des sorties positives.\n",
        "\n",
        "- Etant assez plate, elle influe assez faiblement sur les neurones par rapport à d'autres fonctions d'activations. Le résultat est souvent très proche de 0 ou de 1 causant la saturation de certains neurones.\n",
        "\n",
        "- Elle est couteuse en terme de calcul car elle comprend la fonction exponentielle.\n"
      ],
      "metadata": {
        "id": "hd_1He_kWmWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    a = []\n",
        "    for item in x:\n",
        "        a.append(1/( (1+math.exp(-item) * 1)))\n",
        "        # ici j'ai pri A = 1 : plus A est grand plus on se rapproche à la fonction echelon ...\n",
        "\n",
        "    return a\n",
        "\n",
        "y = sigmoid(x)\n",
        "\n",
        "plt.plot(x,y)\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "scrolled": true,
        "id": "XiQyZuPMWmWG",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tahn :  Tangente Hyperbolique\n",
        "\n",
        "Cette fonction ressemble à la fonction Sigmoïde. La différence avec la fonction Sigmoïde est que la fonction Tanh produit un résultat compris entre -1 et 1. La fonction Tanh est en terme général préférable à la fonction Sigmoïde car elle est centrée sur zéro. Les grandes entrées négatives tendent vers -1 et les grandes entrées positives tendent vers 1.\n",
        "\n",
        "Mis à part cet avantage, la fonction Tanh possède les mêmes autres inconvénients que la fonction Sigmoïde."
      ],
      "metadata": {
        "id": "N6pM0PdtWmWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        return (1 - (x ** 2))\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "y = tanh(x)\n",
        "\n",
        "plt.plot(x,y)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lL-Po6KzWmWH",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReLU : Unité de Rectification Linéaire\n",
        "Pour résoudre le problème de saturation des deux fonctions précédentes (Sigmoïde et Tanh) il existe la fonction ReLU (Unité de Rectification Linéaire). Cette fonction est la plus utilisée.\n",
        "\n",
        "La fonction ReLU est inteprétée par la formule: f(x) = max(0, x). Si l'entrée est négative la sortie est 0 et si elle est positive, alors la sortie est égale à x. Cette fonction d'activation augmente considérablement la convergence du réseau et ne sature pas.\n",
        "\n",
        "Mais la fonction ReLU n'est pas parfaite. Si la valeur d'entrée est négative, le neurone reste inactif, ainsi les poids ne sont pas mis à jour et le réseau n’apprend pas."
      ],
      "metadata": {
        "id": "Su5qkZlWWmWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    a = []\n",
        "    for item in x:\n",
        "        if item > 0:\n",
        "            a.append(item)\n",
        "        else:\n",
        "            a.append(0)\n",
        "    return a\n",
        "\n",
        "\n",
        "y = relu(x)\n",
        "\n",
        "plt.plot(x,y)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NaHM-WDRWmWI",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_77OUTHIO5UQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}