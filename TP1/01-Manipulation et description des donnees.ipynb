{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/CHPS0704/blob/main/TP1/01-Manipulation%20et%20description%20des%20donnees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJtmaERxsTZo"
      },
      "source": [
        "# # Manipulation des données et premières analyses statistiques (EDA)\n",
        "\n",
        "\n",
        "Avant de rentrer dans le détail de ce chapitre, nous allons commencer par charger les packages et importer les données nécessaies aux traitements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUhM6UjwsTZp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHBx-WyRsTZp"
      },
      "source": [
        "### Téléchargement des fichiers nécessaires au cours"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!wget https://github.com/lsteffenel/CHPS0704/raw/main/data/calendar_extrait.csv -O data/calendar_extrait.csv\n",
        "!wget https://github.com/lsteffenel/CHPS0704/raw/main/data/credit.xlsx -O data/credit.xlsx\n",
        "!wget https://github.com/lsteffenel/CHPS0704/raw/main/data/employee-earnings-report-2017.csv -O data/employee-earnings-report-2017.csv\n",
        "!wget https://github.com/lsteffenel/CHPS0704/raw/main/data/listing_extrait.csv -O data/listing_extrait.csv\n",
        "!wget https://github.com/lsteffenel/CHPS0704/raw/main/data/salaries.sqlite -O data/salaries.sqlite"
      ],
      "metadata": {
        "id": "7-4LU3wksa0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0 La librairie Pandas\n",
        "Pandas est une bibliothèque écrite pour le langage de programmation Python permettant la manipulation et l'analyse des données. Nous allons ici détailler l'utilisation des deux structures de base de ce package que sont les Series et DataFrame.\n"
      ],
      "metadata": {
        "id": "lyZFnooAts8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Les objets Series de Pandas\n",
        "On charge le package Pandas"
      ],
      "metadata": {
        "id": "5xJkfkxUtxZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "QcC3eahTt0wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La première structure de Pandas est l'objet **Series**.\n",
        "\n",
        "On peut créer une Series à partir d'une liste:"
      ],
      "metadata": {
        "id": "APTAqaMFt3z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "liste = pd.Series([8, 18, 10, 15])\n",
        "liste"
      ],
      "metadata": {
        "id": "L6jFbc6St6Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Les objets DataFrame de Pandas\n",
        "On peut créer un DataFrame à partir d'une liste ou array:"
      ],
      "metadata": {
        "id": "eqc7_BtXt-O3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "liste = np.array([[ 8,  0],\n",
        "         [18,  7],\n",
        "         [10,  1,],\n",
        "         [14,  3]])\n",
        "df = pd.DataFrame(liste)\n",
        "df"
      ],
      "metadata": {
        "id": "GHB1tzAuuBoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Créer une nouvelle colonne à partir d'une liste"
      ],
      "metadata": {
        "id": "d_Z2eWWruGgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[2]=[\"Paris\", \"Lille\",\"Montpellier\",\"Toulouse\"]\n",
        "df"
      ],
      "metadata": {
        "id": "Hy4fQeWeuHlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Renommer les colonnes:"
      ],
      "metadata": {
        "id": "ZAWQ3XpduL_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns = ['temperature' , 'precipitation', 'ville']\n",
        "df"
      ],
      "metadata": {
        "id": "KjQXdhFwuM3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut également définir une colonne comme index"
      ],
      "metadata": {
        "id": "t8mlG8oHuS8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.set_index(\"ville\")\n",
        "df"
      ],
      "metadata": {
        "id": "BazH3fVAuT3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Manipulation des colonnes\n",
        "\n",
        "Sur un Dataframe, on peut:\n",
        "- selectionner  une colonne avec :"
      ],
      "metadata": {
        "id": "7wo74vpcuYU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['temperature'] # ou df.temperature"
      ],
      "metadata": {
        "id": "rhEg0IMDubZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- construire de nouvelles colonnes à partir de colonnes existantes"
      ],
      "metadata": {
        "id": "Kv9mB91Kucz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"mon_calcul\"] = 2 * df.temperature + df.precipitation\n",
        "df"
      ],
      "metadata": {
        "id": "kl7vzfK1uf6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Manipulation des lignes\n",
        "On extrait une ligne par son index en utilisant :"
      ],
      "metadata": {
        "id": "iQF9onvqukIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[\"Toulouse\"]"
      ],
      "metadata": {
        "id": "fP4ryBIPumy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On extrait une ligne par sa position en utilisant"
      ],
      "metadata": {
        "id": "8JwPQAPxupwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[3]"
      ],
      "metadata": {
        "id": "nFn6aDadusWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On extrait des valeurs à partir de listes :"
      ],
      "metadata": {
        "id": "wQF8MCzruwW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[[\"Toulouse\",\"Paris\"],[\"precipitation\"]]"
      ],
      "metadata": {
        "id": "yz5YRmWBuxH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On extrait des valeurs à partir des index"
      ],
      "metadata": {
        "id": "9rYOjVfRuzZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[1:3,1:2]"
      ],
      "metadata": {
        "id": "VRIVFllfu2hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Les outils pour importer les données\n",
        "\n",
        "Dans le cadre de ce Notebook, pour chaque jeu de données trop volumineux, nous conservons une version avec un extrait des données dans ce répertoire et un lien vers un autre site avec les données complètes (le code pour récupérer ces fichiers volumineux est présent sous forme de commentaire.\n",
        "\n",
        "### Importer des données externes\n",
        "L’une des forces de Pandas est l’importation et l’exportation des données.\n",
        "\n",
        "Ce package possède un ensemble de fonctions très large pour charger des données en mémoire et les exporter dans divers formats. Nous allons développer de nombreux exemples.\n",
        "#### Importer un fichier csv\n",
        "La fonction *read_csv()* de Pandas est une fonction avec un nombre de paramètres impressionnant, nous ne nous concentrons ici que sur quelques-uns qui sont importants.\n",
        "\n",
        "Dans le cas d’un fichier csv classique, un seul paramètre est nécessaire. Il s’agit du chemin vers le fichier. Votre fichier peut se trouver directement sur votre machine mais aussi en ligne. Dans ce cas, il vous suffit de rentrer une adresse web.\n",
        "\n",
        "D’autres paramètres pourront vous être utiles lors du traitement de csv :\n",
        "\n",
        "- *delimiter* : afin de donner le format des séparateurs entre valeurs dans le fichier. Utile dans le cas d’un csv avec des séparateurs points-virgules,\n",
        "- *decimal* : afin de spécifier le séparateur décimal. Utile dans le cas d’un csv avec des séparateurs décimaux utilisant une virgule,\n",
        "- *index_col* : afin de spécifier la position de la colonne servant d’index dans le DataFrame créé (attention les colonnes sont toujours indexées à 0),\n",
        "- *header* : afin de dire si le titre de la colonne se trouve dans la première ligne. Si ce n’est pas le cas, on peut utiliser le paramètre names afin de fournir une liste avec le nom des colonnes pour le DataFrame,\n",
        "- *dtypes* : dans le cas de gros jeux de données, il peut être intéressant de fournir une liste de types de colonnes ou un dictionnaire afin d’éviter à Python d’avoir à les deviner (ce qui vous évitera certains warnings),\n",
        "- de nombreux autres paramètres, notamment sur le traitement des données manquantes, sur la transformation des dates, sur le codage des chaînes de caractères…\n",
        "\n",
        "On utilise *pd.read_csv()* pour lire un fichier csv\n",
        "\n",
        "Dans ce cas, on va récupérer les données des logements AirBnB de Paris."
      ],
      "metadata": {
        "id": "GH1X38Rgu60c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import d'un extrait\n",
        "listing=pd.read_csv(\"./data/listing_extrait.csv\", index_col=0)\n",
        "calendar = pd.read_csv(\"./data/calendar_extrait.csv\",index_col=0)\n",
        "boston = pd.read_csv(\"./data/employee-earnings-report-2017.csv\")\n",
        "# Import du fichier complet\n",
        "#listing=pd.read_csv(\"https://www.stat4decision.com/listing.csv.gz\", index_col=0)\n",
        "listing.head()"
      ],
      "metadata": {
        "id": "HGjnyKK2vJ1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarque** Pour importer un fichier csv très volumineux, il existe deux solutions:\n",
        "- l'option 'chunksize' de read_csv qui permet de lire des morceaux d'un fichier de manière itérative\n",
        "- la librairie **Dask**\n",
        "\n",
        "#### Importer un fichier Excel\n",
        "\n",
        "Microsoft Excel reste l’un des outils de base pour traiter de la donnée. Dans la plupart des projets de data science, vous serez amené à croiser un fichier Excel, que ce soit pour stocker des données ou pour stocker des références ou des informations\n",
        "annexes.\n",
        "\n",
        "Pandas possède des outils pour importer des données en Excel sans avoir à passer\n",
        "par une transformation en csv (souvent fastidieuse si vous avez des classeurs avec de\n",
        "nombreuses feuilles).\n",
        "\n",
        "##### pd.read_excel() :\n",
        "\n",
        "Cette approche ressemble à l’importation en csv. Pour récupérer le fichier Excel, il faut connaître le nom ou la position de la feuille qui nous intéresse :"
      ],
      "metadata": {
        "id": "TXHgIvASvVW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# on peut ajouter le nom de la feuille\n",
        "frame_credit=pd.read_excel(\"./data/credit.xlsx\", sheet_name=\"donnees\")\n",
        "frame_credit.head(5)"
      ],
      "metadata": {
        "id": "o7C96laRvdW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# on peut utiliser uniquement certaines colonnes\n",
        "frame_credit_af=pd.read_excel(\"./data/credit.xlsx\", sheet_name=\"donnees\", usecols=\"A:E\")\n",
        "frame_credit.head(5)"
      ],
      "metadata": {
        "id": "Gl2H9QUyvgzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importer une table issue d’une base de données SQL\n",
        "\n",
        "Le langage SQL  est un langage central de la science des données. La majorité des\n",
        "bases de données relationnelles peuvent être requêtées en utilisant le langage SQL.\n",
        "C’est d’ailleurs aujourd’hui l’un des trois langages les plus utilisés par le data scientist\n",
        "(après Python et R). SQL va vous permettre d’extraire des tables de données qui\n",
        "pourront ensuite être chargées en mémoire dans des DataFrames.\n",
        "\n",
        "Pour passer de la base SQL à Python, il faut donc un connecteur permettant de\n",
        "se connecter à la base et de faire des requêtes directement dessus. Un package\n",
        "central de Python est très utile dans ce but : c’est SQLalchemy qui a aujourd’hui\n",
        "remplacé les nombreux packages spécifiques qui pouvaient exister afin de requêter\n",
        "des bases de données SQL en fonction du type de base : MySQL, PostgreSQL,\n",
        "SQLlite… SQLalchemy a l’avantage de fournir une seule approche.\n",
        "\n",
        "On va utiliser :"
      ],
      "metadata": {
        "id": "NMKUsTQCvkFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sqlalchemy import create_engine, inspect, text as sql_text"
      ],
      "metadata": {
        "id": "eDLmhXEtvryS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ma_con = create_engine(\"sqlite:///data/salaries.sqlite\")\n",
        "inspection = inspect(ma_con)\n"
      ],
      "metadata": {
        "id": "lWTdlbbEvuWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# on peut vérifier le nom des tables\n",
        "inspection.get_table_names()"
      ],
      "metadata": {
        "id": "dGUew2Nbvwq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# on peut charger les données\n",
        "# = pd.read_sql_query(\"SELECT * FROM Salaries\", ma_con)\n",
        "frame_sql = pd.read_sql_query(con=ma_con.connect(), sql=sql_text(\"SELECT * FROM Salaries\"))"
      ],
      "metadata": {
        "id": "2dylK8fMvywa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_sql.head()"
      ],
      "metadata": {
        "id": "KqeUfuQ0v0xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ5j1_IssTZq"
      },
      "source": [
        "## 2 Décrire et transformer des colonnes\n",
        "### 2 Décrire la structure de vos données\n",
        "Quel que soit le type de structure que vous utilisez ; les arrays, les Series ou\n",
        "les DataFrame, on utilise généralement une propriété de ces objets : la propriété\n",
        "    .shape. Celle-ci renvoie toujours un tuple, qui aura autant d’éléments que de dimensions\n",
        "dans vos données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frslbR8esTZq"
      },
      "outputs": [],
      "source": [
        "calendar.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wpCBipGsTZq"
      },
      "source": [
        "Cette information est importante mais reste peu détaillée. Lorsqu’on travaille sur\n",
        "un DataFrame, on va chercher à avoir beaucoup plus de détails. Pour cela, nous allons\n",
        "utiliser la méthode .info(). Si nous prenons le jeu de données des occupations des\n",
        "logements AirBnB, nous aurons :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvCdVpIbsTZr"
      },
      "outputs": [],
      "source": [
        "calendar.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRhI4jjYsTZr"
      },
      "source": [
        "Une autre étape importante est l’étude de l’aspect de notre DataFrame, on peut\n",
        "par exemple afficher les premières lignes du jeu de données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "89Z1z5zvsTZr"
      },
      "outputs": [],
      "source": [
        "calendar.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tsEvdfQsTZr"
      },
      "source": [
        "Une autre propriété importante des DataFrame de Pandas est .columns. En effet, celle-ci a deux utilités :\n",
        "\n",
        "- afficher le nom des colonnes de votre DataFrame,\n",
        "- créer une structure permettant d’avoir une liste des colonnes que nous pourrons utiliser pour des automatisations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zNrslmbsTZr"
      },
      "outputs": [],
      "source": [
        "calendar.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1wu7GgRsTZr"
      },
      "outputs": [],
      "source": [
        "# on peut faire une boucle sur les colonnes de notre DataFrame\n",
        "for col in calendar.columns:\n",
        "    print(col, calendar[col].dtype, sep=\" : \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njEQfxGWsTZr"
      },
      "source": [
        "### Quelles transformations pour les colonnes de vos données ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjKdPkH0sTZr"
      },
      "source": [
        "Votre objectif en tant que data scientist est d’extraire le plus d’information possible de ces données. Pour cela, il va falloir les mettre en forme de manière intelligente.\n",
        "\n",
        "Nous allons étudier différentes transformations nécessaires pour travailler sur des données :\n",
        "\n",
        "- les changements de types,\n",
        "- les jointures,\n",
        "- la discrétisation,\n",
        "- le traitement de données temporelles,\n",
        "- les transformations numériques,\n",
        "- le traitement des colonnes avec des données qualitatives,\n",
        "- le traitement des données manquantes,\n",
        "- la construction de tableaux croisés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJfLub_9sTZr"
      },
      "source": [
        "### Les changements de types\n",
        "\n",
        "Le typage des colonnes d’un DataFrame ou d’un array est très important pour tous les traitements en data science.\n",
        "\n",
        "Nous nous concentrons ici sur les structures en DataFrame de Pandas. Pandas va automatiquement inférer les types si vous ne lui avez pas spécifié de type à l’importation\n",
        "des données ou à la création du DataFrame.\n",
        "\n",
        "Par défaut, Pandas va utiliser trois types principaux :\n",
        "- les entiers int en 32 ou en 64 bits,\n",
        "- les nombres décimaux float en 32 ou 64 bits,\n",
        "- les objets object qui rassemblent la plupart des autres types.\n",
        "\n",
        "On trouvera aussi des booléens et tous les types définis par NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCPQWc2hsTZr"
      },
      "source": [
        "La base de données listing de AirBnB est obtenue par scrapping web et certaines informations ne peuvent pas être traitées directement. En effet, lorsqu’on affiche les\n",
        "informations sur les colonnes, on voit que la colonne price est typée en Object alors qu’il s’agit de valeurs décimales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9bepsgAsTZr"
      },
      "outputs": [],
      "source": [
        "listing[\"price\"].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoJ43qIWsTZr"
      },
      "source": [
        "Pour nous débarrasser du $ en première position, nous avons trois possibilités :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Y8QjVbbsTZr"
      },
      "outputs": [],
      "source": [
        "# élimine le premier élément\n",
        "%timeit listing[\"price\"].str[1 :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT2Tj_vHsTZs"
      },
      "outputs": [],
      "source": [
        "# remplace tous les $\n",
        "%timeit listing[\"price\"].str.replace(\"$\",\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQgFxKDqsTZs"
      },
      "outputs": [],
      "source": [
        "#élimine le premier élément lorsque c’est un $\n",
        "%timeit listing[\"price\"].str.strip(\"$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM7UI1HosTZs"
      },
      "source": [
        "On voit que ces trois approches sont assez différentes, la première est la plus\n",
        "efficace en termes de temps de calcul mais elle est aussi la plus dangereuse en cas\n",
        "d’erreur dans nos données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vWVWeFtsTZs"
      },
      "source": [
        "Il reste deux étapes à réaliser : éliminer les virgules et transformer la variable en\n",
        "variable numérique :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L_I7NtFsTZs"
      },
      "outputs": [],
      "source": [
        "listing[\"price\"]= pd.to_numeric(listing[\"price\"].str.strip(\"$\").str.replace(\",\",\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHMmD0ZRsTZs"
      },
      "outputs": [],
      "source": [
        "print(listing[\"price\"].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euLbh5X5sTZs"
      },
      "source": [
        "Nous avons donc réussi à modifier notre colonne.\n",
        "\n",
        "Si nous désirons automatiser ce traitement, il suffit de créer une boucle sur les colonnes. On utilise le code suivant :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjoMxggPsTZs"
      },
      "outputs": [],
      "source": [
        "list_error = []\n",
        "for col in listing.columns:\n",
        "    if listing[col].dtype==object :\n",
        "        try:\n",
        "            listing[col]= pd.to_numeric(listing[col].str.strip(\"$\")\\\n",
        "                                                .str.replace(\",\", \"\"))\n",
        "        except ValueError:\n",
        "            list_error.append(col)\n",
        "print(f\"Les colonnes {' '.join(list_error)} n'ont pas pu être transformées\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0xtFUFLsTZs"
      },
      "source": [
        "On a utilisé une gestion d'exception pour ne transformer que les colonnes qui nous intéressent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-V1TNMasTZs"
      },
      "source": [
        "Si on étudie la colonne \"instant_bookable\", on veut pouvoir prendre en compte cette colonne pour la passer en booléen :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rwm0vYdsTZs"
      },
      "outputs": [],
      "source": [
        "# approche avec NumPy\n",
        "listing[\"instant_bookable_bool\"] = np.where(\n",
        "    listing[\"instant_bookable\"]==\"f\",\n",
        "    False,\n",
        "    True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6sxDzzKsTZs"
      },
      "outputs": [],
      "source": [
        "# approche avec un dictionnaire et Pandas\n",
        "listing[\"instant_bookable_bool2\"]= listing[\"instant_bookable\"].replace(\n",
        "    {\"f\" : False,\"t\" : True}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXTCfx_isTZs"
      },
      "source": [
        "On voit dans ce code que lorsqu’on veut remplacer deux valeurs, l’utilisation de la\n",
        "fonction np.where de NumPy peut être une solution, mais il faut être attentif aux\n",
        "risques liés à des mauvais codages de la variable.\n",
        "\n",
        "Il existe de nombreux cas de nettoyages de données basées sur des erreurs de\n",
        "typage. Ce que nous allons voir dans tout ce chapitre pourra vous aider à répondre à\n",
        "vos problématiques spécifiques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU_G4SqzsTZs"
      },
      "source": [
        "### Les jointures et concaténations\n",
        "#### Les jointures entre DataFrame\n",
        "\n",
        "Les jointures entre DataFrame sont un outil puissant de Pandas qui ressemble\n",
        "aux outils disponibles en SQL. Une jointure consiste à construire, à partir de deux\n",
        "DataFrame, un DataFrame en utilisant ce qu’on appelle une clé de jointure qui sera\n",
        "un identifiant des lignes présent dans les deux DataFrame initiaux."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvJkH8PlsTZt"
      },
      "source": [
        "La fonction de jointure de Pandas est la fonction pd.merge(). Elle prend comme\n",
        "paramètres deux objets DataFrame puis des paramètres optionnels :\n",
        "- on : choix de la ou des clés de jointure.\n",
        "- how : choix de la méthode de jointure. Il faut choisir entre left, right, inner et outer.\n",
        "- left_on (et right_on) : si les clés de jointure n’ont pas le même nom d’une table à une autre.\n",
        "- index_left (et index_right) : on donnera ici un booléen si l’index du DataFrame est utilisé comme clé.\n",
        "\n",
        "Sur les données AirBnB, nous utiliserons une jointure interne afin d’associer les\n",
        "calendriers des  logements :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hzyx-VssTZt"
      },
      "outputs": [],
      "source": [
        "global_airbnb = pd.merge(listing,\n",
        "                         calendar,\n",
        "                         left_index=True,\n",
        "                         right_index=True,\n",
        "                         how=\"inner\"\n",
        "                        )\n",
        "global_airbnb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slsqI-k1sTZt"
      },
      "source": [
        "On voit ici qu’on a rassemblé les colonnes des deux DataFrame. Dans ce cas, le\n",
        "DataFrame calendar est beaucoup plus grand que listing, le DataFrame obtenu ne\n",
        "rassemble que les clés communes aux deux DataFrame mais lorsqu’il y a plusieurs\n",
        "répétitions d’une clé, la combinaison est répétée."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ESc6Q7hsTZx"
      },
      "source": [
        "### La gestion des duplications de lignes\n",
        "\n",
        "Il arrive souvent dans des données que des lignes soient dupliquées par erreur ou que vous désiriez vérifier la duplication de certaines lignes.\n",
        "\n",
        "Pandas possède deux outils pour traiter ce type de données : duplicated() et drop_duplicated().\n",
        "\n",
        "Si nous voulons vérifier si des lignes sont dupliquées dans le DataFrame sur les employés de la ville de Boston, il nous suffit de faire :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJqoBVRVsTZy"
      },
      "outputs": [],
      "source": [
        "boston.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FgOrkz-sTZy"
      },
      "source": [
        "Il s’avère qu’il n’y a aucune duplication. Nous aurions pu nous concentrer uniquement\n",
        "sur le nom, le département et le titre des employés :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWYSnCwQsTZy"
      },
      "outputs": [],
      "source": [
        "boston.duplicated(\n",
        "    ['NAME','DEPARTMENT NAME','TITLE']).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuckqI7esTZy"
      },
      "outputs": [],
      "source": [
        "# on a donc quatre éléments dupliqués, on peut maintenant les visualiser :\n",
        "boston[boston.duplicated(\n",
        "    ['NAME','DEPARTMENT NAME','TITLE'],\n",
        "    keep=False\n",
        ")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk5fAKH_sTZy"
      },
      "source": [
        "Nous pouvons maintenant nous débarrasser des duplications, on utilisera pour cela :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU4HcLU5sTZy"
      },
      "outputs": [],
      "source": [
        "boston_no_dup = boston.drop_duplicates(\n",
        "    ['NAME','DEPARTMENT NAME','TITLE'],\n",
        "    keep=\"first\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7yHUmJesTZy"
      },
      "source": [
        "\n",
        "Dans ce cas, on garde le premier. On peut demander à garder le dernier (last) et\n",
        "on utilisera des tris afin d’ordonner les résultats pour se débarrasser des duplications\n",
        "non pertinentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPzkKhYMsTZy"
      },
      "source": [
        "### La discrétisation\n",
        "La discrétisation permet de transformer une variable quantitative (l’âge des\n",
        "individus par exemple) en une variable qualitative (une classe d’âge pour chaque individu). Pour cela, nous utilisons deux fonctions de Pandas : pd.cut() et\n",
        "pd.qcut()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-itZgeYvsTZy"
      },
      "source": [
        "#### Intervalles constants\n",
        "Si nous désirons créer une variable de classe basée sur des intervalles de taille\n",
        "constante allant du minimum au maximum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdZd8zd8sTZy"
      },
      "outputs": [],
      "source": [
        "listing[\"price_disc1\"] = pd.cut(listing[\"price\"],\n",
        "                                bins=5\n",
        "                               )\n",
        "listing[\"price_disc1\"].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh_5eoYLsTZy"
      },
      "source": [
        "On voit ici que la nouvelle variable a comme valeurs les intervalles.\n",
        "\n",
        "Si vous voulez vérifier la répartition par intervalle, il suffit d’utiliser la méthode *.value_counts()* :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXJTq2iZsTZy"
      },
      "outputs": [],
      "source": [
        "listing[\"price_disc1\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkpy1AJksTZy"
      },
      "source": [
        "Par ailleurs, si vous désirez donner des noms aux intervalles, vous pouvez le faire en utilisant le paramètre *labels=* de la fonction *cut()* :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfhIYsIusTZz"
      },
      "outputs": [],
      "source": [
        "listing[\"price_disc1\"]=pd.cut(listing[\"price\"],\n",
        "                              bins=5,\n",
        "                              labels=range(5)\n",
        "                             )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEeJ62Q6sTZz"
      },
      "source": [
        "#### Intervalles définis par l’utilisateur\n",
        "Si vous désirez créer des intervalles sur mesure, il vous suffit de donner les bornes\n",
        "de ces intervalles. On utilise :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSTq8I4XsTZz"
      },
      "outputs": [],
      "source": [
        "listing[\"price_disc2\"]=pd.cut(\n",
        "    listing[\"price\"],\n",
        "    bins=[listing[\"price\"].min(),\n",
        "          50,\n",
        "          100,\n",
        "          500,\n",
        "          listing[\"price\"].max()\n",
        "         ],\n",
        "    include_lowest = True)\n",
        "listing[\"price_disc2\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3KMn3WOsTZz"
      },
      "source": [
        "On remplace donc le nombre d’intervalles par une liste de valeurs (ici on prend le minimum et le maximum des données).\n",
        "\n",
        "Afin d’inclure le minimum, on ajoute *include_lowest=True*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pjxj29RsTZz"
      },
      "source": [
        "#### Intervalles de fréquence constante\n",
        "\n",
        "Il est souvent intéressant de construire des intervalles ayant un nombre d’individus constant d’une classe à une autre.\n",
        "\n",
        "Pour cela, on va utiliser une autre fonction de Pandas nommée *qcut()*. Elle prend le même type de paramètres que la\n",
        "fonction précédente mais elle va créer des classes de taille similaire (en nombre d’individus) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2V5fgRNsTZz"
      },
      "outputs": [],
      "source": [
        "listing[\"price_disc3\"]=pd.qcut(listing[\"price\"],\n",
        "                               q=5\n",
        "                              )\n",
        "listing[\"price_disc3\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oURGODbrsTZz"
      },
      "source": [
        "Pandas a fait de son mieux pour bien distribuer les données dans les intervalles.\n",
        "Comme il y a beaucoup de prix égaux, il n’a pas pu obtenir des intervalles avec des\n",
        "fréquences parfaitement égales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmrpHJ0asTZz"
      },
      "source": [
        "### Les tris\n",
        "Les tris sont des outils importants en data science. Il vous arrive très fréquemment de vouloir trier des données. Chaque package possède des outils de tris, nous allons\n",
        "en étudier deux : celui de NumPy et celui de Pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_uXL0ChsTZz"
      },
      "source": [
        "#### Le tri de NumPy\n",
        "Si nous restons sur un array de NumPy dans son sens le plus classique, celui-ci contient une méthode .sort() qui s’applique très bien sur un array à une seule dimension, on pourra avoir :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yy8C5eRsTZz"
      },
      "outputs": [],
      "source": [
        "array1 = np.random.randn(5000)\n",
        "array1.sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQVQRh4-sTZz"
      },
      "source": [
        "Cette méthode modifie l’array1 et trie de manière croissante.\n",
        "\n",
        "Si on désire faire un tri décroissant, on pourra utiliser :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_25aSjPCsTZz"
      },
      "outputs": [],
      "source": [
        "array1[::-1].sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njxGcg6-sTZz"
      },
      "source": [
        "Comme vous le voyez, cette méthode n’est pas très efficace pour faire des tris\n",
        "complexes. On utilisera une autre méthode nommée .argsort() :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XPSbv8tsTZz"
      },
      "outputs": [],
      "source": [
        "table = np.random.rand(5000, 10)\n",
        "table[table[:,1].argsort()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0nK_whFsTZz"
      },
      "source": [
        "On trie donc sur la seconde colonne de notre array. On peut alors retourner le\n",
        "résultat de ce tri.\n",
        "Le tri basé sur .argsort() est extrêmement efficace mais s’applique avant tout\n",
        "à un array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZDBqe4SsTZ0"
      },
      "source": [
        "#### Le tri de Pandas\n",
        "Pandas possède une fonction de tri sur les DataFrames extrêmement efficace qui se rapproche beaucoup d’une approche SQL des tris. Elle a de nombreux paramètres et permet de trier sur plusieurs clés dans des sens différents.\n",
        "\n",
        "Si nous prenons nos données sur les logements AirBnB, nous désirons trier les données par ordre croissant de nombre de chambres, puis par niveau de prix décroissant.\n",
        "\n",
        "Pour cela, une seule ligne de code est nécessaire :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOCbgqE9sTZ0"
      },
      "outputs": [],
      "source": [
        "# on affiche uniquement les 5 premières lignes du résultat\n",
        "listing.sort_values([\"bedrooms\",\"price\"], ascending=[True, False]).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avJuGOsUsTZ0"
      },
      "source": [
        "On a donc bien un outil puissant basé sur des listes de clés. Comme dans le cas des jointures avec Pandas, lorsqu’on a plusieurs variables ou paramètres de tri, on les place dans une liste.\n",
        "\n",
        "Par défaut, le tri de Pandas trie par colonne avec le paramètre *axis=1*. Si vous désirez trier par ligne, vous pouvez changer ce paramètre.\n",
        "\n",
        "Pandas vous permet aussi d’effectuer des tris sur les index en utilisant .sort_index().\n",
        "\n",
        "L’outil de tri de Pandas est moins performant en termes de rapidité d’exécution que le *.argsort()* de NumPy. Néanmoins, les possibilités plus grandes et le fait de travailler sur une structure plus complexe, telle que le DataFrame, nous confortent\n",
        "dans l’utilisation du tri de Pandas pour nos analyses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fuDuvjRsTZ0"
      },
      "source": [
        "### Le traitement de données temporelles\n",
        "Python a de nombreux outils pour travailler sur des dates, notamment le package\n",
        "datetime nativement présent dans Python.\n",
        "\n",
        "#### Les dates avec NumPy\n",
        "\n",
        "Depuis peu, il est possible de travailler avec des dates à l’intérieur d’un array de NumPy (depuis NumPy 1.7). Ainsi la fonction np.datetime64 permet de créer des dates, et le type datetime est utilisable pour créer des arrays. On peut par exemple\n",
        "utiliser *arange()* pour générer une suite de semaines de janvier 2017 à janvier 2018 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOtz2-jHsTZ0"
      },
      "outputs": [],
      "source": [
        "np.arange(\"2017-01-01\",\"2018-01-01\", dtype=\"datetime64[W]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJoUAd07sTZ0"
      },
      "source": [
        "Il existe de nombreuses fonctions permettant de travailler sur les dates, notamment avec les différences basées sur la fonction timedelta().\n",
        "On peut aussi travailler sur les jours travaillés (business days). Cette partie de NumPy est en constante évolution. La documentation de NumPy est le meilleur outil pour en suivre les avancées."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvSNF1qDsTZ0"
      },
      "source": [
        "#### Les dates avec Pandas\n",
        "C'est clairement Pandas qui a l'ascendant sur le traitement des dates en data science. Avec des fonctions efficaces et simples à prendre en main, le travail sur les dates est extrêmement simplifié.\n",
        "\n",
        "Pandas possède de sérieux atouts dans la prise en compte des dates notamment avec l’intégration des formats de dates dans l’importation des données. Néanmoins, si vos données n’ont pas été correctement importées, il est très simple de transformer des chaînes de caractères dans un DataFrame ou dans une Series en dates. Pour cela, on utilise :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8geJNpOqsTZ0"
      },
      "outputs": [],
      "source": [
        "pd.to_datetime(['11/12/2017', '05-01-2018'],\n",
        "               dayfirst=True,\n",
        "               format='mixed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nRMGqqEsTZ0"
      },
      "source": [
        "On crée ainsi un DatetimeIndex qui peut être utilisé dans une Series ou dans\n",
        "un DataFrame. On peut aussi donner un format de dates en utilisant le paramètre *format=*.\n",
        "\n",
        "Il est souvent intéressant de traiter de nombreuses dates. On a très souvent envie de générer des suites de dates de manière automatique. Imaginons que nous avons des données quotidiennes de cotation d’un indice boursier, et que nous désirons\n",
        "transformer ces données en une série indexée sur les jours ouvrés pendant lesquels la banque est ouverte. Les données pour l’année sont stockées dans un array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zQtMl0ssTZ0"
      },
      "outputs": [],
      "source": [
        "index_ouverture = pd.bdate_range('2017-01-01','2017-12-31')\n",
        "data = np.random.randn(index_ouverture.size)\n",
        "pd.Series(data, index=index_ouverture).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfYVogOjsTZ0"
      },
      "source": [
        "On peut aussi utiliser date_range() avec différents paramètres. Si par exemple,\n",
        "on désire générer un index avec des relevés toutes les 2 heures entre le 1er février\n",
        "2018 à 8h00 et le 31 mars 2018 à 8h00, on utilisera :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmEWPGT8sTZ0"
      },
      "outputs": [],
      "source": [
        "index_temps = pd.date_range('2018-02-01 08:00:00','2018-03-31 08:00:00', freq='2h')\n",
        "print(index_temps.shape, index_temps.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9pLXRydsTZ1"
      },
      "source": [
        "De nombreuses possibilités sont accessibles pour le traitement des dates et des heures.\n",
        "\n",
        "Ainsi, si plutôt que des dates et des heures, vous préférez utiliser des périodes (ceci revient à utiliser un mois plutôt que le premier jour du mois comme valeur de votre index), vous pouvez le faire avec la fonction period_range()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw61wqKqsTZ1"
      },
      "outputs": [],
      "source": [
        "pd.period_range(\"01-01-2017\",\"01-01-2018\", freq=\"M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh4qA7XssTZ1"
      },
      "source": [
        "On a ainsi généré une suite de mois. Ceci peut se faire sur des semaines (W), des trimestres (Q), des années (A)…\n",
        "\n",
        "Si on désire générer des périodes, on pourra le faire grâce à pd.period() :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hrbMZrzsTZ1"
      },
      "outputs": [],
      "source": [
        "pd.period_range(pd.Period(\"2017-01\", freq=\"M\"), pd.Period(\"2019-01\", freq=\"M\"), freq=\"Q\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_zHlC_NsTZ1"
      },
      "source": [
        "Par ailleurs, vous pouvez traiter les fuseaux horaires de manière simplifiée avec\n",
        "Pandas en utilisant la propriété .tz. Par défaut, une date n’est associée à aucune\n",
        "timezone :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHZ4yhBSsTZ1"
      },
      "outputs": [],
      "source": [
        "index_temps.tz is None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NfcbJEdsTZ1"
      },
      "source": [
        "Pour définir un fuseau horaire, on le fait généralement dans la fonction date_range(), qui a un paramètre tz = . Les fuseaux horaires peuvent être définis, avec\n",
        "une chaîne de caractères incluant une combinaison zone/ville (\"Europe/Paris\"), vous pouvez en obtenir la liste exhaustive en important :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e8pJ-3msTZ1"
      },
      "outputs": [],
      "source": [
        "from pytz import common_timezones, all_timezones\n",
        "all_timezones[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Int_bE-IsTZ1"
      },
      "source": [
        "Si vous avez déjà défini vos dates et que vous désirez leur ajouter un fuseau horaire, vous allez utiliser la méthode tz_localize(). Imaginons que l’on génère\n",
        "des données toutes les deux heures à Paris, on veut transformer cet index en passant sur le fuseau horaire de Nouméa en Nouvelle-Calédonie, voici le code :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VsZ28gAsTZ1"
      },
      "outputs": [],
      "source": [
        "index_heures=pd.date_range(\"2018-01-01 09:00:00\", \"2018-01-01 18:00:00\", freq=\"2h\")\n",
        "index_heures_paris = index_heures.tz_localize(\"Europe/Paris\")\n",
        "index_heures_paris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otT8iaansTZ1"
      },
      "outputs": [],
      "source": [
        "index_heures_noumea = index_heures_paris.tz_convert(\"Pacific/Noumea\")\n",
        "index_heures_noumea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfmkOu77sTZ1"
      },
      "source": [
        "Il existe de nombreux autres outils pour travailler sur les dates en Python.\n",
        "Notamment, lorsqu’on traite des séries temporelles, on peut utiliser l’outil rolling :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtvCF0DhsTZ1"
      },
      "outputs": [],
      "source": [
        "pd.Series(data, index=index_ouverture).plot()\n",
        "pd.Series(data, index=index_ouverture).rolling(window=10).mean().plot()\n",
        "#La deuxième ligne permet d’afficher la moyenne prise sur 10 points adjacents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYTaLjolsTZ1"
      },
      "source": [
        "### Le traitement des données manquantes\n",
        "Les données manquantes sont un domaine de la data science à part entière. Leur traitement nécessite une réflexion bien au-delà de quelques lignes de codes.\n",
        "\n",
        "Dans tous vos projets data science, vous serez confronté à des données manquantes, elles sont réparties en trois types principaux :\n",
        "\n",
        "- Les données manquantes complètement aléatoirement\n",
        "- Les données manquantes aléatoirement\n",
        "- Les données manquantes non aléatoirement\n",
        "\n",
        "Ainsi, pour les deux premiers cas, on pourra penser à des méthodes d’imputation\n",
        "alors que pour le troisième il ne sera pas possible de faire cela."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iNJhjVKsTZ1"
      },
      "source": [
        "#### Les données manquantes en Python\n",
        "NumPy possède un code standard pour gérer les données manquantes, il s’agit\n",
        "de NaN. On peut définir un élément d’un array comme une donnée manquante en\n",
        "utilisant :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rQxShgCsTZ2"
      },
      "outputs": [],
      "source": [
        "table = np.random.rand(5000, 10)\n",
        "table[0,1] = np.nan\n",
        "table[0,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91j5LUhnsTZ2"
      },
      "source": [
        "L’avantage d’utiliser ce codage réside dans le fait que les nan n’altèrent pas le type\n",
        "de votre array et qu’ils ne sont pas pris en compte dans les calculs de statistiques\n",
        "descriptives avec les fonctions adaptées :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMxH8iuHsTZ2"
      },
      "outputs": [],
      "source": [
        "vec = np.ones(10)\n",
        "vec[3] = np.nan\n",
        "np.nansum(vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U3o2ztgsTZ2"
      },
      "source": [
        "Lorsque vous importez des données avec Pandas, celui-ci va automatiquement\n",
        "remplacer les données manquantes par des codes nan.\n",
        "\n",
        "#### La suppression des données manquantes\n",
        "L’approche la plus simple pour traiter des données manquantes est de supprimer les observations comportant des données manquantes.\n",
        "\n",
        "Pandas comporte de nombreuses méthodes pour cela. Si nous prenons les données sur les salaires des employés de la ville de Boston, nous pouvons utiliser :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx5TKG8SsTZ2"
      },
      "outputs": [],
      "source": [
        "# la table globale\n",
        "boston.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJH8N5_hsTZ2"
      },
      "outputs": [],
      "source": [
        "# la table lorsqu’on retire les lignes avec données manquantes\n",
        "boston.dropna().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rnp5WVMcsTZ2"
      },
      "outputs": [],
      "source": [
        "# la table lorsqu’on retire les colonnes avec des données manquantes\n",
        "boston.dropna(axis = 1).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTOK3GzwsTZ2"
      },
      "source": [
        "On voit que dans cette table de nombreuses données manquantes existent surtout sur huit colonnes. Quatre colonnes sont complètes.\n",
        "\n",
        "#### La complétion par la moyenne, le mode ou la médiane\n",
        "\n",
        "Avant de compléter nos données, il va falloir transformer nos données Boston de manière à avoir des données numériques. En s’inspirant du code vu plus haut pour les données AirBnB, nous pouvons faire cela avec :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxdFLwMFsTZ2"
      },
      "outputs": [],
      "source": [
        "for col in boston.columns :\n",
        "    if boston[col].dtype==object :\n",
        "        boston[col]=pd.to_numeric(boston[col].str.replace(r\"\\(.*\\)\",\"\")\\\n",
        "                                  .str.replace(\",\",\"\").str.strip(\"$\"),\n",
        "                                  errors='ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEYYszBFsTZ2"
      },
      "source": [
        "Dans ce code, on supprime d’abord les parenthèses en utilisant une expression régulière (voir le chapitre 2), puis on élimine les virgules et on enlève du sigle $ lorsqu’il est en début de chaîne.\n",
        "\n",
        "On a maintenant huit colonnes en float avec des salaires.\n",
        "\n",
        "On peut maintenant travailler sur les données manquantes. Il existe deux moyens de compléter par la moyenne ou par la médiane.\n",
        "\n",
        "Un premier en utilisant Pandas :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltQ9CRl4sTZ2"
      },
      "outputs": [],
      "source": [
        "# pour la moyenne\n",
        "for col in boston.select_dtypes(\"number\").columns :\n",
        "    boston[col] = boston[col].fillna(boston[col].mean())\n",
        "\n",
        "\n",
        "# pour la médiane\n",
        "for col in boston.select_dtypes(\"number\").columns :\n",
        "    boston[col] = boston[col].fillna(boston[col].median())\n",
        "\n",
        "# pour le mode, on utilise une condition à l’intérieur de l’appel de la boucle\n",
        "# qui est équivalente à ce que nous faisions plus haut\n",
        "# le calcul du mode renvoie un objet Series et non une valeur comme les autres\n",
        "# méthodes, d’où le [0]\n",
        "for col in boston.select_dtypes(object).columns :\n",
        "    boston[col] = boston[col].fillna(boston[col].mode()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWnxuyhGsTZ2"
      },
      "source": [
        "Le package Scikit-Learn permet aussi de faire des remplacements par la moyenne\n",
        "ou la médiane :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdsXGkinsTZ2"
      },
      "outputs": [],
      "source": [
        "# à partir de sklearn version 0.20\n",
        "from sklearn.impute import SimpleImputer\n",
        "# on crée un objet de cette classe avec la stratégie d’imputation comme\n",
        "# paramètre\n",
        "imputer = SimpleImputer(strategy = \"mean\")\n",
        "# on construit un nouveau jeu de données en appliquant la méthode\n",
        "# .fit_transform()\n",
        "boston_imputee2 = imputer.fit_transform(boston.select_dtypes(\"number\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfBUF_OlsTZ2"
      },
      "source": [
        "### Le traitement des colonnes avec des données qualitatives\n",
        "Les données qualitatives sont extrêmement présentes dans les données. Dès que vous travaillez sur des données socio-démographiques sur des individus, vous allez rencontrer des données qualitatives. Le traitement des données qualitatives est souvent négligé dans les ouvrages de traitement de la donnée. Il est donc primordial de bien expliquer le traitement qu’elles requièrent.\n",
        "\n",
        "#### Le type categorical\n",
        "Les données qualitatives sont des valeurs textuelles par défaut. Pandas propose un type spécifique pour traiter ce type de données. Le type categorical permet d’optimiser le traitement de ce type de données.\n",
        "\n",
        "Il permet de créer et de transformer des données de ce type. Vous avez importé des données avec des variables qualitatives, Pandas va automatiquement les considérer comme du type object. Vous pourrez le voir en utilisant la propriété .dtype.\n",
        "Si vous désirez transformer ce type en un type categorical, vous pouvez utiliser la fonction pd.Categorical() :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWLVdzNRsTZ3"
      },
      "outputs": [],
      "source": [
        "var_quali = pd.Categorical([\"Boston\",\"Paris\",\"Londres\",\"Paris\", \"Boston\"])\n",
        "var_quali"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA7jpsxKsTZ3"
      },
      "outputs": [],
      "source": [
        "# on peut ajouter une modalité\n",
        "var_quali = var_quali.add_categories(\"Rome\")\n",
        "# on alloue cette valeur à un élément de notre objet\n",
        "var_quali[4] = \"Rome\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAlAIf2hsTZ3"
      },
      "source": [
        "Si on modifie notre objet en y ajoutant une modalité non définie au préalable alors on aura un message d’erreur.\n",
        "Si on veut transformer une colonne objet en category, on utilise :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIgpIO92sTZ3"
      },
      "outputs": [],
      "source": [
        "boston[\"POSTAL\"]=boston[\"POSTAL\"].astype(\"category\")\n",
        "#équivalent sans warning\n",
        "boston[\"POSTAL\"]=boston[\"POSTAL\"].astype(pd.api.types.CategoricalDtype(ordered=False))\n",
        "\n",
        "boston[\"POSTAL\"].dtype\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfN7S8jfsTZ3"
      },
      "source": [
        "On utilise ordered = False car il n’y a pas d’ordre entre les modalités de notre colonne. Si une notion d’ordre doit être ajoutée, on ajoute la liste des modalités et on\n",
        "passe le paramètre ordered à True.\n",
        "\n",
        "Le type Categorical est inspiré du type factor de R."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCrUNff8sTZ3"
      },
      "source": [
        "#### La transformation des données\n",
        "Pour traiter des données qualitatives, il faudra les transformer.\n",
        "\n",
        "En effet, les algorithmes que vous aurez à utiliser sont basés sur des données numériques et donc des variables quantitatives.\n",
        "\n",
        "Si vous travaillez sur des données nominales, il va falloir transformer les variables en indicatrices. C’est-à-dire que vous allez obtenir une colonne pour chaque modalité de votre variable qualitative.\n",
        "\n",
        "Cette approche peut être appliquée avec deux packages que nous utilisons souvent : Pandas et Scikit-Learn.\n",
        "\n",
        "Dans le cadre de nos données sur les logements AirBnB, nous avons plusieurs variables qualitatives, notamment roomtype qui a trois modalités :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CjdlowZsTZ3"
      },
      "outputs": [],
      "source": [
        "listing[\"room_type\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhgz0XvhsTZ3"
      },
      "source": [
        "##### Approche Pandas avec get_dummies() :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIdR20bCsTZ3"
      },
      "outputs": [],
      "source": [
        "frame_room_type = pd.get_dummies(listing[\"room_type\"])\n",
        "frame_room_type.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VmNdUT_sTZ3"
      },
      "source": [
        "Cette fonction crée un nouveau DataFrame\n",
        "\n",
        "##### Approche Scikit-Learn avec OneHotEncoder() :\n",
        "\n",
        "Dans ce cas, il faut que la variable qualitative soit déjà sous forme d’entiers entre 0 et p-1, p étant le nombre de modalités de notre variable.\n",
        "\n",
        "On va combiner deux classes de Scikit-Learn : LabelEncoder et OneHotEncoder.\n",
        "\n",
        "La première va permettre de recoder les valeurs textuelles en entiers et la seconde de construire des colonnes binaires à partir des valeurs de la variable transformée.\n",
        "\n",
        "Voici le code :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeDWNrS7sTZ3"
      },
      "outputs": [],
      "source": [
        "# on importe les classes\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "# on crée un objet de la classe LabelEncoder\n",
        "encode1=LabelEncoder()\n",
        "# on crée un objet de la classe OneHotEncoder\n",
        "encode2=OneHotEncoder(sparse_output= False)\n",
        "# on combine l’application des deux classes\n",
        "array_out=encode2.fit_transform(encode1.fit_transform(listing[\"room_type\"]).reshape(-1,1))\n",
        "# on transforme la sortie en DataFrame\n",
        "pd.DataFrame(array_out, columns=listing[\"room_type\"].unique()).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXDyvaB7sTZ3"
      },
      "source": [
        "**Attention à partir de la version 0.20 de Scikit Learn, OneHotEncoder peut gérer des données non numériques, on a alors :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHHEWiNjsTZ3"
      },
      "outputs": [],
      "source": [
        "# on importe les classes\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# on crée un objet de la classe OneHotEncoder\n",
        "encode=OneHotEncoder(sparse_output= False,)\n",
        "# on l'applique directement sur la colonnes initiale\n",
        "array_out=encode.fit_transform(np.array(listing[\"room_type\"]).reshape(-1,1))\n",
        "# on transforme la sortie en DataFrame\n",
        "pd.DataFrame(array_out, columns=listing[\"room_type\"].unique()).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsgkHB4LsTZ4"
      },
      "source": [
        "Si vous travaillez sur des données ordinales (avec des modalités ordonnées), il vous suffit de recoder une variable avec des valeurs chiffrées (attention, cette approche\n",
        "n’est valable que pour des données ordinales).\n",
        "\n",
        "##### Approche avec Pandas :\n",
        "\n",
        "Il n’y a pas d’approche automatisée, on peut utiliser le code suivant :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n6g8xLLsTZ4"
      },
      "outputs": [],
      "source": [
        "listing[\"room_type2\"]=listing[\"room_type\"].map(dict(zip(listing[\"room_type\"]\\\n",
        "                                                        .unique(),\n",
        "                                                        range(listing[\"room_type\"].nunique())) ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-j_EllCsTZ4"
      },
      "source": [
        "On crée donc une colonne en utilisant un dictionnaire qui permet de faire correspondre\n",
        "des éléments de celui-ci à des valeurs entières.\n",
        "\n",
        "##### Approche avec Scikit-Learn :\n",
        "\n",
        "Cette approche est plus simple, elle se base sur l’outil LabelEncoder et se fait avec\n",
        "ce code :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFr1O9xGsTZ4"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encode1 = LabelEncoder()\n",
        "listing[\"room_type2\"] = encode1.fit_transform(listing[\"room_type\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIVGw7JysTZ4"
      },
      "source": [
        "Ces méthodes sont centrales car la plupart des algorithmes en Python ne\n",
        "supportent pas le type categorical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiWQ2hs2sTZ4"
      },
      "source": [
        "### Les transformations numériques\n",
        "\n",
        "Lorsque vous travaillez sur des données, un certain nombre de transformations de base sont nécessaires. Trois packages pourront être utiles pour ce type de transformations : Scikit-Learn, Pandas et SciPy.\n",
        "\n",
        "Avec Pandas, la plupart des transformations se font en faisant les calculs directement en utilisant les fonctions universelles de Pandas.\n",
        "\n",
        "Avec Scikit-Learn, l’approche est légèrement différente. Dans ce cas, on utilise des classes permettant de transformer les données.\n",
        "\n",
        "SciPy nous permet d’appliquer des transformations plus spécifiques.\n",
        "\n",
        "Nous utiliserons les données sur les employés de la ville de Boston desquelles nous extrayons les colonnes numériques :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC4oFPq8sTZ4"
      },
      "outputs": [],
      "source": [
        "boston_num=boston.select_dtypes(include=[\"number\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEsOOh_vsTZ4"
      },
      "source": [
        "#### Centrer et réduire les données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJz2Y7I8sTZ4"
      },
      "outputs": [],
      "source": [
        "# avec Pandas pour centrer et réduire\n",
        "boston_std=boston_num.apply(lambda x : (x-x.mean())/x.std())\n",
        "boston_std.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COS2ipcHsTZ4"
      },
      "outputs": [],
      "source": [
        "# avec Scikit-Learn, on utilisera la classe StandardScaler :\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "rescaled = scaler.fit_transform(boston_num)\n",
        "pd.DataFrame(rescaled, index=boston_num.index, columns=boston_num.columns).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v49pgr8PsTZ4"
      },
      "source": [
        "#### Changer d’échelle\n",
        "\n",
        "On utilise pour passer à une échelle 0-100 :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI7ezI9gsTZ4"
      },
      "outputs": [],
      "source": [
        "# avec Pandas\n",
        "boston_0_100 = boston_num.apply(lambda x : (x-x.min())/(x.max()-x.min())*100)\n",
        "boston_0_100.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM0oOl2esTZ4"
      },
      "outputs": [],
      "source": [
        "# avec Scikit-Learn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "minmaxscaler=MinMaxScaler((0,100))\n",
        "boston_0_100=minmaxscaler.fit_transform(boston_num)\n",
        "boston_0_100=pd.DataFrame(boston_0_100, index=boston_num.index, columns=boston_num.columns)\n",
        "boston_0_100.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqBONaUisTZ4"
      },
      "source": [
        "#### Transformation de Box-Cox\n",
        "Lorsque vous désirez obtenir des données suivant une loi normale, vous risquez d’avoir besoin d’une transformation qui permette de se rapprocher de cette loi.\n",
        "\n",
        "La transformation de Box-Cox est une transformation qui peut gérer ce problème.\n",
        "\n",
        "Elle ne s’applique qu’à des données positives. Celle-ci est disponible dans le package SciPy et s’utilise de la manière suivante :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G7k6ei1sTZ5"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "total_earning_trans = stats.boxcox(\n",
        "    boston_num[\"TOTAL EARNINGS\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F41KzFunsTZ5"
      },
      "source": [
        "### Echantillonnage des données\n",
        "\n",
        "Nous allons présenter deux approches d’échantillonnage :\n",
        "\n",
        "- l'échantillonnage aléatoire sans remise,\n",
        "- l'échantillonnage stratifié.\n",
        "\n",
        "Pandas propose une méthode d’échantillonnage simple à mettre en oeuvre, il s’agit de la méthode sample.\n",
        "Si on désire échantillonner aléatoirement 1000 lignes de notre base Boston, on utilisera :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0_6IiPYsTZ5"
      },
      "outputs": [],
      "source": [
        "boston_aleat_1000 = boston.sample(n=1000)\n",
        "boston_aleat_1000.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on_cinTqsTZ5"
      },
      "source": [
        "On peut se servir du paramètre frac = si on désire obtenir un échantillon de la taille d’une fraction du jeu de données initial.\n",
        "\n",
        "L’échantillonnage stratifié consiste à reproduire dans vos échantillons les mêmes répartitions de certaines variables que dans l’échantillon initial.\n",
        "\n",
        "Il peut se faire avec Pandas ou avec Scikit-Learn. On utilisera dans ce cas le listing des logements disponibles à Paris.\n",
        "\n",
        "Si nous voulons échantillonner 10 % des logements en conservant la répartition du type de chambre (room_type), on utilisera :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmAZ8O6GsTZ5"
      },
      "outputs": [],
      "source": [
        "# répartition dans l’échantillon initial\n",
        "listing[\"room_type\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pi8zwud4sTZ5"
      },
      "outputs": [],
      "source": [
        "# échantillonnage stratifié\n",
        "listing_sample = listing.groupby('room_type').apply(lambda x : x.sample(frac=.1))\n",
        "# repartition dans l’échantillon final\n",
        "listing_sample[\"room_type\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2BrK8jJsTZ5"
      },
      "source": [
        "### La construction de tableaux croisés\n",
        "\n",
        "Les tableaux croisés peuvent être très utiles pour visualiser des croisements de colonnes de variables qualitatives et les intégrer dans d’autres calculs.\n",
        "Deux fonctions dans Pandas sont utiles : la méthode frame.pivot_table() et la fonction pd.crosstab().\n",
        "\n",
        "La seule différence entre ces deux approches réside dans les données qui sont acceptées dans chaque fonction. Pour pivot_table, sachant que c’est une méthode\n",
        "appliquée à un DataFrame, toutes les données doivent venir de ce DataFrame. La fonction crosstab est différente, elle peut prendre en entrée des données issues\n",
        "de plusieurs DataFrame ou d’arrays.\n",
        "\n",
        "Si nous reprenons nos données AirBnB et que nous désirons croiser deux colonnes,\n",
        "nous allons utiliser :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8M8R2EssTZ5"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(\n",
        "    listing['instant_bookable'],\n",
        "    listing['room_type']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DPEh4-isTZ5"
      },
      "source": [
        "Par défaut, cet outil inclut des comptages dans le tableau. Mais si on désirait afficher la moyenne du prix des logements, on utiliserait :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zbsd8DVksTZ5"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(listing['instant_bookable'],\n",
        "            listing['room_type'],\n",
        "            values=listing['price'],\n",
        "            aggfunc=\"mean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UqHSlI0sTZ5"
      },
      "outputs": [],
      "source": [
        "listing.pivot_table(values='price',\n",
        "                    index='instant_bookable',\n",
        "                    columns='room_type',\n",
        "                    aggfunc='mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usdbadmhsTZ5"
      },
      "source": [
        "On peut aller plus loin en combinant plusieurs variables et en combinant plusieurs statistiques dans le tableau :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhMVcOrNsTZ5"
      },
      "outputs": [],
      "source": [
        "listing.pivot_table(values='price',\n",
        "                    index=['host_is_superhost','instant_bookable'],\n",
        "                    columns='room_type',\n",
        "                    aggfunc=['mean','count'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVVDQhk4sTZ5"
      },
      "source": [
        "## 3 Extraire des statistiques descriptives\n",
        "\n",
        "### Statistiques pour données quantitatives\n",
        "\n",
        "Lorsqu'on calcule des statistiques descriptives spécifiques aux données quantitatives sur un DataFrame complet, Pandas n’affiche des résultats que pour les variables quantitatives (sans message d’erreur pour les colonnes non quantitatives).\n",
        "\n",
        "Statistiques descriptives de base\n",
        "Quelques méthodes statistiques universelles de Pandas :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e57Zh-osTZ5"
      },
      "outputs": [],
      "source": [
        "# moyenne\n",
        "boston.select_dtypes(\"number\").mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGn6qTITsTZ6"
      },
      "outputs": [],
      "source": [
        "# variance\n",
        "boston.select_dtypes(\"number\").var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9lbbYG9sTZ6"
      },
      "outputs": [],
      "source": [
        "# écart-type\n",
        "boston.select_dtypes(\"number\").std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knW63esysTZ6"
      },
      "outputs": [],
      "source": [
        "# médiane\n",
        "boston.select_dtypes(\"number\").median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw8e81ZSsTZ6"
      },
      "outputs": [],
      "source": [
        "# matrice de corrélation\n",
        "boston.select_dtypes(\"number\").corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19C4UwQzsTZ6"
      },
      "source": [
        "Une autre fonction intéressante est la méthode .describe() qui affiche un certain nombre de statistiques pour les variables quantitatives (elle ne fait que cela par défaut mais nous verrons plus loin qu’elle peut s’appliquer aux variables qualitatives)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5UrnPJXsTZ6"
      },
      "outputs": [],
      "source": [
        "boston.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSDfzqW2sTZ6"
      },
      "source": [
        "Si vous voulez construire votre propre DataFrame de statistiques, vous pouvez utiliser la méthode .agg(). Par exemple :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1Hs5D2FsTZ6"
      },
      "outputs": [],
      "source": [
        "boston.select_dtypes(\"number\").agg([\"mean\",\"std\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBmrctzsTZ6"
      },
      "source": [
        "#### Des statistiques plus avancées\n",
        "Il peut arriver que des statistiques plus avancées soient nécessaires, notamment en se basant sur des distributions de probabilités. Pour cela, on utilisera plutôt le\n",
        "package SciPy et plus précisément scipy.stats qui possède de nombreuses statistiques importantes.\n",
        "\n",
        "Par exemple, on peut calculer l’asymétrie d’une distribution (skewness) en utilisant :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBXIsrWbsTZ6"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import skew\n",
        "skew(listing[\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPPnKYnBsTZ6"
      },
      "source": [
        "### Statistiques pour données qualitatives\n",
        "Les statistiques descriptives pour des variables qualitatives sont très différentes de celles pour des variables quantitatives. Ainsi, on s’intéresse généralement au mode et à la fréquence des modalités de la variable, Pour cela, on pourra obtenir des statistiques simples en utilisant la méthode *.describe(include = \"all\")*.\n",
        "\n",
        "D’autres approches sont possibles mais elles s’appliqueront variables par variables sur un objet Series. Ainsi, on peut utiliser :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjtfoKOLsTZ6"
      },
      "outputs": [],
      "source": [
        "# nombre de modalités\n",
        "listing[\"room_type\"].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "movekVc-sTZ6"
      },
      "outputs": [],
      "source": [
        "# liste des modalités\n",
        "listing[\"room_type\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKO0YC1EsTZ6"
      },
      "outputs": [],
      "source": [
        "# liste et fréquence d’apparition des modalités\n",
        "listing[\"room_type\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc1QMVYisTZ6"
      },
      "outputs": [],
      "source": [
        "# calcul du mode\n",
        "listing[\"room_type\"].mode()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDWnhlInsTZ7"
      },
      "source": [
        "Ces méthodes vont compter le nombre de modalité, afficher toutes les modalités, afficher les modalités ordonnées par fréquence d’apparition avec la fréquence associée,et enfin afficher le mode (la modalité avec la fréquence la plus élevée).\n",
        "\n",
        "La méthode .value_counts() possède un certain nombre de paramètres pour inclure les données manquantes, normaliser les résultats..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeQmO359sTZ7"
      },
      "source": [
        "## 4 Utilisation du groupby pour décrire des données\n",
        "\n",
        "### Le principe\n",
        "\n",
        "La méthode .groupby est une méthode qui permet de construire un objet à partir d’un DataFrame. Cet objet sépare les données en fonction des modalités d’une ou de plusieurs variables qualitatives. On obtiendra ainsi de manière quasi-immédiate des\n",
        "indicateurs par modalités.\n",
        "\n",
        "De nombreuses méthodes sont disponibles sur ces objets groupby afin de maximiser la simplicité de manipulation de données.\n",
        "Généralement, on suppose que le groupby est basé sur trois étapes : séparation/application et combinaison.\n",
        "\n",
        "Par exemple, sur les données AirBnB, on peut faire cela par type de chambres :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T28MLO4YsTZ7"
      },
      "outputs": [],
      "source": [
        "listing_group_room = listing.groupby(\"room_type\")\n",
        "listing_group_room[\"price\"].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag9ler5asTZ7"
      },
      "source": [
        "On sépare et on calcule la moyenne, et on rassemble les résultats dans un nouvel objet. On affiche donc dans un objet Series les prix moyens par type de chambre. On voit ici qu’on a utilisé la méthode .mean() de la classe des objets groupby."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2geF5tFEsTZ7"
      },
      "source": [
        "### Les opérations sur les objets groupby\n",
        "On peut très simplement obtenir des statistiques plus poussées avec des groupby.\n",
        "\n",
        "De nombreuses méthodes de transformation de données pourront être appliquées avec une étape .groupby()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_LgydHDsTZ7"
      },
      "source": [
        "### Apply : une méthode importante pour manipuler vos groupby\n",
        "\n",
        "La méthode apply permet d’appliquer n’importe quelle fonction sur vos données.\n",
        "\n",
        "Si par exemple, vous désirez calculer l’écart salarial au sein de chaque département sur les données des salariés de la ville de Boston, vous allez devoir utiliser la différence entre le maximum et le minimum. Il n’existe pas de fonction universelle.\n",
        "\n",
        "Nous allons donc utiliser un groupby et la méthode apply :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps-jass7sTZ7"
      },
      "outputs": [],
      "source": [
        "diff_salaires_dep = boston.groupby('DEPARTMENT NAME')['TOTAL EARNINGS']\\\n",
        "                        .apply(lambda x : x.max()-x.min())\n",
        "diff_salaires_dep.sort_values(ascending = False).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs_c1ASasTZ7"
      },
      "source": [
        "### Cas concret d’utilisation d’un groupby\n",
        "\n",
        "Nous travaillons sur les données AirBnB. Nous désirons obtenir des statistiques descriptives sur les prix et leurs variations au sein de chaque arrondissement de Paris. Pour cela nous allons utiliser un groupby :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hft3q0_sTZ7"
      },
      "outputs": [],
      "source": [
        "listing.neighbourhood_cleansed.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMfM0x0usTZ7"
      },
      "outputs": [],
      "source": [
        "# on construit des statistiques par quartier\n",
        "listing.groupby(\"neighbourhood_cleansed\")[\"price\"].agg(\n",
        "    [\"mean\",\"std\",\"count\"]).sort_values(by=\"mean\",\n",
        "                                        ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgRqQsRJsTZ7"
      },
      "source": [
        "Si nous désirons étudier les variations par arrondissement et par type d’appartement, nous pourrons avoir deux clés pour notre groupby :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clGYTWamsTZ7"
      },
      "outputs": [],
      "source": [
        "listing.groupby([\"neighbourhood_cleansed\",\"room_type\"])[\"price\"].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxOKMnQ_sTZ7"
      },
      "source": [
        "De nombreuses autres applications sont disponibles avec le groupby."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remerciements\n",
        "ce notebook est largement inspiré du notebook proposé par https://github.com/emjako/pythondatascientist"
      ],
      "metadata": {
        "id": "eb30QgIVw5A3"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}